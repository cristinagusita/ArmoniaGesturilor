{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "igMyGnjE9hEp"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2HDvhIu9hEr"
      },
      "source": [
        "# Specify each path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "9NvZP2Zn9hEy"
      },
      "outputs": [],
      "source": [
        "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
        "model_save_path = 'model/keypoint_classifier/keypoint_classifier.hdf5'\n",
        "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5oMH7x19hEz"
      },
      "source": [
        "# Set number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "du4kodXL9hEz"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjnL0uso9hEz"
      },
      "source": [
        "# Dataset reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QT5ZqtEz9hE0"
      },
      "outputs": [],
      "source": [
        "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "QmoKFsp49hE0"
      },
      "outputs": [],
      "source": [
        "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Augmenting the data via rotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rotate_point(x, y, cx, cy, angle):\n",
        "    rad = math.radians(angle)\n",
        "    x_new = (x - cx) * math.cos(rad) - (y - cy) * math.sin(rad) + cx\n",
        "    y_new = (x - cx) * math.sin(rad) + (y - cy) * math.cos(rad) + cy\n",
        "    return x_new, y_new\n",
        "\n",
        "def rotate_normalized_landmarks(landmarks, angle, max_value):\n",
        "    # Assuming landmarks are normalized and in pairs of (x, y)\n",
        "    landmarks = [[landmarks[i], landmarks[i+1]] for i in range(0, len(landmarks), 2)]\n",
        "\n",
        "    # Inverse Normalization (if needed)\n",
        "    landmarks = [[x * max_value, y * max_value] for x, y in landmarks]\n",
        "\n",
        "    # Apply rotation\n",
        "    rotated_landmarks = []\n",
        "    cx, cy = 0, 0  # Assuming the first point is the base (0,0) after normalization\n",
        "    for x, y in landmarks:\n",
        "        x_new, y_new = rotate_point(x, y, cx, cy, angle)\n",
        "        rotated_landmarks.append([x_new, y_new])\n",
        "\n",
        "    # Flatten and Re-Normalize\n",
        "    rotated_landmarks = list(itertools.chain.from_iterable(rotated_landmarks))\n",
        "    rotated_landmarks = [n / max_value for n in rotated_landmarks]\n",
        "\n",
        "    return rotated_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def rotate_landmarks(landmarks, angle, center=None):\n",
        "    \"\"\"\n",
        "    Rotate landmarks by a given angle around a center point.\n",
        "\n",
        "    :param landmarks: List of (x, y) tuples representing the landmarks.\n",
        "    :param angle: Rotation angle in degrees.\n",
        "    :param center: Tuple (x, y) representing the center of rotation. If None, the center of the landmarks is used.\n",
        "    :return: Rotated landmarks.\n",
        "    \"\"\"\n",
        "    if center is None:\n",
        "        # Calculate the center of all landmarks\n",
        "        xs, ys = zip(*landmarks)\n",
        "        center = (sum(xs) / len(landmarks), sum(ys) / len(landmarks))\n",
        "\n",
        "    # Convert angle from degrees to radians\n",
        "    angle_rad = math.radians(angle)\n",
        "\n",
        "    # Rotation matrix\n",
        "    rotation_matrix = np.array([\n",
        "        [math.cos(angle_rad), -math.sin(angle_rad)],\n",
        "        [math.sin(angle_rad), math.cos(angle_rad)]\n",
        "    ])\n",
        "\n",
        "    # Rotate each landmark\n",
        "    rotated_landmarks = []\n",
        "    for x, y in landmarks:\n",
        "        # Translate point to origin\n",
        "        temp_x, temp_y = x - center[0], y - center[1]\n",
        "\n",
        "        # Rotate point\n",
        "        rotated_x, rotated_y = np.dot(rotation_matrix, [temp_x, temp_y])\n",
        "\n",
        "        # Translate point back\n",
        "        rotated_x, rotated_y = rotated_x + center[0], rotated_y + center[1]\n",
        "\n",
        "        rotated_landmarks.append((rotated_x, rotated_y))\n",
        "\n",
        "    return rotated_landmarks\n",
        "\n",
        "def on_trackbar_change(dummy=None):\n",
        "    # Get current positions of all trackbars\n",
        "    tilt_value = cv2.getTrackbarPos(\"Rotation Angle\", \"Landmarks\") \n",
        "    shear_value_x = cv2.getTrackbarPos(\"Shear_X\", \"Landmarks\") \n",
        "    shear_value_y = cv2.getTrackbarPos(\"Shear_Y\", \"Landmarks\")\n",
        "    # Apply tilt to the landmarks\n",
        "    tilted_landmarks = tilt_keypoints(landmarks, rotation_angle=tilt_value, shear_angle_x=shear_value_x, shear_angle_y=shear_value_y)\n",
        "\n",
        "    # Clear image and redraw landmarks\n",
        "    image[:] = (0, 0, 0)\n",
        "    draw_landmarks(image, tilted_landmarks)\n",
        "    cv2.imshow(\"Landmarks\", image)\n",
        "\n",
        "def draw_landmarks(image, landmarks, color=(0, 255, 0), radius=5):\n",
        "    for (x, y) in landmarks:\n",
        "        cv2.circle(image, (int(x), int(y)), radius, color, -1)\n",
        "    return image\n",
        "\n",
        "def tilt_keypoints(keypoints, rotation_angle, shear_angle_x=0, shear_angle_y=0, center=None):\n",
        "    \"\"\"\n",
        "    Apply a tilt (rotation followed by shear) to keypoints.\n",
        "\n",
        "    :param keypoints: List of (x, y) tuples representing keypoints.\n",
        "    :param rotation_angle: Rotation angle in degrees.\n",
        "    :param shear_angle_x: Shear angle along x-axis in degrees.\n",
        "    :param shear_angle_y: Shear angle along y-axis in degrees.\n",
        "    :param center: A tuple (x, y) representing the center of rotation and shear. \n",
        "                   If None, the mean of keypoints is used.\n",
        "    :return: List of tilted keypoints.\n",
        "    \"\"\"\n",
        "    # Convert angles from degrees to radians\n",
        "    rotation_rad = np.radians(rotation_angle)\n",
        "    shear_rad_x = np.radians(shear_angle_x)\n",
        "    shear_rad_y = np.radians(shear_angle_y)\n",
        "\n",
        "    # Rotation matrix\n",
        "    R = np.array([[np.cos(rotation_rad), -np.sin(rotation_rad)], \n",
        "                  [np.sin(rotation_rad), np.cos(rotation_rad)]])\n",
        "\n",
        "    # Shear matrix\n",
        "    S = np.array([[1, np.tan(shear_rad_x)], \n",
        "                  [np.tan(shear_rad_y), 1]])\n",
        "\n",
        "    # Combined transformation matrix\n",
        "    T = np.dot(S, R)\n",
        "\n",
        "    # If no center is provided, use the mean of keypoints\n",
        "    if center is None:\n",
        "        center = np.mean(keypoints, axis=0)\n",
        "\n",
        "    # Translate keypoints to origin, apply transformation, and translate back\n",
        "    tilted_keypoints = [T.dot(kp - center) + center for kp in keypoints]\n",
        "\n",
        "    return tilted_keypoints\n",
        "\n",
        "image_width, image_height = 640, 480\n",
        "\n",
        "# Original landmarks\n",
        "landmark_list = [[413, 405], [343, 390], [289, 341], [253, 298], [220, 267], [352, 233], [335, 171], [328, 132], [325, 97], [396, 223], [388, 151], [382, 107], [377, 71], [437, 232], [443, 165], [443, 125], [441, 88], [477, 256], [494, 211], [501, 182], [504, 154]]\n",
        "landmarks = [(x, y) for x, y in landmark_list]\n",
        "\n",
        "# Create a black image\n",
        "image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "\n",
        "# Initialize the window\n",
        "cv2.namedWindow(\"Landmarks\")\n",
        "\n",
        "# Maximum tilt angle for the slider\n",
        "max_tilt = 360\n",
        "max_shear_X = 360\n",
        "max_shear_Y = 360\n",
        "\n",
        "# Create a trackbar (slider) in the window\n",
        "cv2.createTrackbar(\"Rotation Angle\", \"Landmarks\", 0, max_tilt, on_trackbar_change)\n",
        "cv2.createTrackbar(\"Shear_X\", \"Landmarks\", 0, max_shear_X, on_trackbar_change)\n",
        "cv2.createTrackbar(\"Shear_Y\", \"Landmarks\", 0, max_shear_Y, on_trackbar_change)\n",
        "\n",
        "# Initialize the image with original landmarks\n",
        "draw_landmarks(image, landmarks)  # Use 'landmarks' (list of tuples), not 'landmark_list' (list of lists)\n",
        "cv2.imshow(\"Landmarks\", image)\n",
        "\n",
        "# Wait until a key is pressed to exit\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment_landmarks(landmarks):\n",
        "    augmented_landmarks = []\n",
        "\n",
        "    # Iterate through the specified ranges\n",
        "    for rotation_angle in range(0, 361, 30):  # [0, 360] with a step of 10\n",
        "        for shear_x in range(-20, 21, 10):      # [-20, 20] with a step of 5\n",
        "            for shear_y in range(-20, 21, 10):  # [-20, 20] with a step of 5\n",
        "                if rotation_angle != 360 and (rotation_angle != 0 and shear_x != 0 and shear_y != 0):\n",
        "                    # Apply the tilt transformation\n",
        "                    tilted_landmarks = tilt_keypoints(landmarks, rotation_angle, shear_x, shear_y)\n",
        "                    tilted_landmarks = np.array([arr.tolist() for arr in tilted_landmarks])\n",
        "                    \n",
        "\n",
        "                    # add guassian noise\n",
        "                    noise = np.random.normal(scale=0.1, size=tilted_landmarks.shape)\n",
        "\n",
        "                    tilted_landmarks = tilted_landmarks + noise\n",
        "\n",
        "                    # convert to list\n",
        "                    tilted_landmarks = [arr.tolist() for arr in tilted_landmarks]\n",
        "\n",
        "                    # Add the result to the list of augmented landmarks\n",
        "                    augmented_landmarks.append(tilted_landmarks)\n",
        "\n",
        "    return augmented_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import itertools\n",
        "def pre_process_landmark(landmark_list):\n",
        "    # This function does the following:\n",
        "    # Relative Positioning: By normalizing landmarks relative to a specific point on the hand (like the wrist), you account for the hand's position in the image. \n",
        "    # This makes the landmarks' positions relative to each other, rather than to the whole image.\n",
        "    # Scale Invariance: Normalizing the size of landmarks ensures that the gesture recognition is scale-invariant. \n",
        "    # That means the size of the hand (whether it's close to the camera or far away) doesn't affect the recognition process.\n",
        "    # Consistency Across Different Inputs: This process helps in maintaining consistency of the landmark \n",
        "    # data across different frames or different hands, making the gesture recognition more robust and reliable.\n",
        "\n",
        "    landmark_list = landmark_list.reshape(-1, 2).tolist()\n",
        "\n",
        "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
        "\n",
        "    # Convert to relative coordinates\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, landmark_point in enumerate(temp_landmark_list):\n",
        "        if index == 0:\n",
        "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
        "\n",
        "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
        "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
        "\n",
        "    # Convert to a one-dimensional list\n",
        "    temp_landmark_list = list(\n",
        "        itertools.chain.from_iterable(temp_landmark_list))\n",
        "\n",
        "    # Normalization\n",
        "    max_value = max(list(map(abs, temp_landmark_list)))\n",
        "\n",
        "    def normalize_(n):\n",
        "        return n / max_value\n",
        "\n",
        "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
        "\n",
        "    return temp_landmark_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize landmarks\n",
        "# new_dataset_X_preprocessed = np.array(list(map(pre_process_landmark, X_dataset)))\n",
        "new_dataset_X_preprocessed = X_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxK_lETT9hE0"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vHBmUf1t9hE1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "def initialize_backbone(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(128, activation='tanh', input_shape=input_shape),\n",
        "        Dense(64, activation='tanh'),\n",
        "        Dense(32, activation='tanh'),\n",
        "\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_combined_model(input_shape, num_classes, backbone):\n",
        "    # Backbone for feature extraction\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "    processed_a = backbone(input_a)\n",
        "    processed_b = backbone(input_b)\n",
        "\n",
        "    # Contrastive Head\n",
        "    distance = Lambda(lambda tensors: K.sqrt(K.sum(K.square(tensors[0] - tensors[1]), axis=1, keepdims=True)))([processed_a, processed_b])\n",
        "\n",
        "\n",
        "    # Classification Head\n",
        "    classification_output = Dense(num_classes, activation='softmax')(processed_a)\n",
        "\n",
        "    # Combined Model\n",
        "    combined_model = Model(inputs=[input_a, input_b], outputs=[distance, classification_output])\n",
        "    \n",
        "    # Classification Model (for inference)\n",
        "    classification_model = Model(inputs=input_a, outputs=classification_output)\n",
        "\n",
        "    return combined_model, classification_model\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    # print shapes\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    margin = 1 \n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "# def combined_loss(y_true, y_pred, alpha=0.5, beta=0.5):\n",
        "#     c_loss = contrastive_loss(y_true[0], y_pred[0])\n",
        "#     # print shapes\n",
        "#     print(y_true[1].shape)\n",
        "#     print(y_pred[1].shape)\n",
        "#     class_loss = sparse_categorical_crossentropy(y_true[1], y_pred[1])\n",
        "#     return alpha * c_loss + beta * class_loss\n",
        "\n",
        "input_shape = (21 * 2,)  # Example input shape\n",
        "num_classes = NUM_CLASSES  # Define NUM_CLASSES appropriately\n",
        "\n",
        "# Initialize Backbone\n",
        "backbone = initialize_backbone(input_shape)\n",
        "\n",
        "# Create Combined and Classification Models\n",
        "combined_model, classification_model = create_combined_model(input_shape, num_classes, backbone)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MbMjOflQ9hE1"
      },
      "outputs": [],
      "source": [
        "# Model checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_path, verbose=1, save_weights_only=False)\n",
        "# Callback for early stopping\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='dense_3_loss', mode='min', patience=5, restore_best_weights=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 42)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 42)]                 0         []                            \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 32)                   15840     ['input_1[0][0]',             \n",
            "                                                                     'input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 1)                    0         ['sequential[0][0]',          \n",
            "                                                                     'sequential[1][0]']          \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 11)                   363       ['sequential[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16203 (63.29 KB)\n",
            "Trainable params: 16203 (63.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Show summary\n",
        "combined_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 42)]              0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 32)                15840     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 11)                363       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16203 (63.29 KB)\n",
            "Trainable params: 16203 (63.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classification_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c3Dac0M_9hE2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "combined_model.compile(optimizer='adam',\n",
        "                       loss={'lambda': contrastive_loss, \n",
        "                             'dense_3': 'sparse_categorical_crossentropy'},\n",
        "                       loss_weights={'lambda': 0.3, \n",
        "                                     'dense_3': 0.7},\n",
        "                       metrics={'dense_3': 'accuracy'},\n",
        "                       run_eagerly=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_pairs(keypoints, labels):\n",
        "    pair_labels = []\n",
        "    pairs = []\n",
        "    classification_labels = []\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
        "\n",
        "    for idx, label in enumerate(labels):\n",
        "        # Add classification label\n",
        "        classification_labels.append(label)\n",
        "\n",
        "        # Positive Pair (same label)\n",
        "        positive_indices = label_indices[label]\n",
        "        positive_pair_idx = np.random.choice(positive_indices[positive_indices != idx])\n",
        "        pairs.append([keypoints[idx], keypoints[positive_pair_idx]])\n",
        "        pair_labels.append(1)\n",
        "\n",
        "        # Negative Pair (different label)\n",
        "        negative_label = np.random.choice(unique_labels[unique_labels != label])\n",
        "        negative_pair_idx = np.random.choice(label_indices[negative_label])\n",
        "        pairs.append([keypoints[idx], keypoints[negative_pair_idx]])\n",
        "        pair_labels.append(0)\n",
        "\n",
        "    duplicated_classification_labels = []\n",
        "    for label in labels:\n",
        "        duplicated_classification_labels.extend([label, label])  # Duplicate label for positive and negative pair\n",
        "\n",
        "    return np.array(pairs), np.array(pair_labels), np.array(duplicated_classification_labels)\n",
        "\n",
        "# Example usage\n",
        "pairs, pair_labels, classification_labels = create_pairs(new_dataset_X_preprocessed, y_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split pairs and pair labels with stratification over classification labels\n",
        "pairs_train, pairs_test, pair_labels_train, pair_labels_test, classification_labels_train, classification_labels_test = train_test_split(\n",
        "    pairs, pair_labels, classification_labels, test_size=0.5, stratify=classification_labels, random_state=42)\n",
        "\n",
        "# Splitting the pairs into two inputs for the model\n",
        "input_a_train, input_b_train = pairs_train[:, 0], pairs_train[:, 1]\n",
        "input_a_test, input_b_test = pairs_test[:, 0], pairs_test[:, 1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "a = augment_landmarks(input_a_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given a list [a,b,c,d,e,f], transform it to [[a,b], [c,d], [e,f]]\n",
        "\n",
        "def transform_to_pairs(input_list):\n",
        "    return [input_list[i:i+2] for i in range(0, len(input_list), 2)]\n",
        "\n",
        "# Reverse of the above function\n",
        "def transform_to_list(input_list):\n",
        "    return [elem for pair in input_list for elem in pair]\n",
        "\n",
        "\n",
        "a = augment_landmarks(transform_to_pairs(X_dataset[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_input_a_train = []\n",
        "new_classification_labels_train = []\n",
        "new_pair_labels_train = []\n",
        "i = 0\n",
        "for landmark in input_a_train:\n",
        "    a = augment_landmarks(transform_to_pairs(landmark.tolist()))\n",
        "    a = [transform_to_list(elem) for elem in a]\n",
        "    new_input_a_train.append(landmark.tolist())\n",
        "    new_input_a_train.extend(a)\n",
        "    # Add the same classification label for the augmented landmarks\n",
        "    new_classification_labels_train.extend([classification_labels_train[i]] * (len(a) + 1))\n",
        "    # Do the same for pair labels\n",
        "    new_pair_labels_train.extend([pair_labels_train[i]] * (len(a) + 1))\n",
        "    i = i + 1\n",
        "  \n",
        "new_input_a_train = np.array(new_input_a_train)\n",
        "new_classification_labels_train = np.array(new_classification_labels_train)\n",
        "new_pair_labels_train = np.array(new_pair_labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_input_b_train = []\n",
        "for landmark in input_b_train:\n",
        "    a = augment_landmarks(transform_to_pairs(landmark.tolist()))\n",
        "    a = [transform_to_list(elem) for elem in a]\n",
        "    new_input_b_train.append(landmark.tolist())\n",
        "    new_input_b_train.extend(a)\n",
        "  \n",
        "new_input_b_train = np.array(new_input_b_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess inputs\n",
        "new_input_a_train = [pre_process_landmark(x) for x in new_input_a_train]\n",
        "new_input_b_train = [pre_process_landmark(x) for x in new_input_b_train]\n",
        "\n",
        "new_input_a_train = np.array(new_input_a_train)\n",
        "new_input_b_train = np.array(new_input_b_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess test\n",
        "new_input_a_test = [pre_process_landmark(x) for x in input_a_test]\n",
        "new_input_b_test = [pre_process_landmark(x) for x in input_b_test]\n",
        "\n",
        "new_input_a_test = np.array(new_input_a_test)\n",
        "new_input_b_test = np.array(new_input_b_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(False, False, False, False, False, False, False, False)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check for nan values in all the arrays\n",
        "np.isnan(new_input_a_train).any(), np.isnan(new_input_b_train).any(), np.isnan(new_input_a_test).any(), np.isnan(new_input_b_test).any(), np.isnan(new_classification_labels_train).any(), np.isnan(new_pair_labels_train).any(), np.isnan(pair_labels_test).any(), np.isnan(classification_labels_test).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(False, False, False, False, False, False, False, False)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look for infinity\n",
        "np.isinf(new_input_a_train).any(), np.isinf(new_input_b_train).any(), np.isinf(new_input_a_test).any(), np.isinf(new_input_b_test).any(), np.isinf(new_classification_labels_train).any(), np.isinf(new_pair_labels_train).any(), np.isinf(pair_labels_test).any(), np.isinf(classification_labels_test).any()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XI0j1Iu9hE2"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WirBl-JE9hE3",
        "outputId": "71b30ca2-8294-4d9d-8aa2-800d90d399de",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "249/249 [==============================] - ETA: 0s - loss: 1.6808 - lambda_loss: 0.1463 - dense_3_loss: 2.3384 - dense_3_accuracy: 0.1767\n",
            "Epoch 1: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 22s 87ms/step - loss: 1.6808 - lambda_loss: 0.1463 - dense_3_loss: 2.3384 - dense_3_accuracy: 0.1767 - val_loss: 1.8751 - val_lambda_loss: 0.1248 - val_dense_3_loss: 2.6252 - val_dense_3_accuracy: 0.1667\n",
            "Epoch 2/20\n",
            "  1/249 [..............................] - ETA: 18s - loss: 1.6276 - lambda_loss: 0.1549 - dense_3_loss: 2.2588 - dense_3_accuracy: 0.1953"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Cristina\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "249/249 [==============================] - ETA: 0s - loss: 1.2633 - lambda_loss: 0.3184 - dense_3_loss: 1.6683 - dense_3_accuracy: 0.4664\n",
            "Epoch 2: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 19s 78ms/step - loss: 1.2633 - lambda_loss: 0.3184 - dense_3_loss: 1.6683 - dense_3_accuracy: 0.4664 - val_loss: 1.2448 - val_lambda_loss: 0.2899 - val_dense_3_loss: 1.6540 - val_dense_3_accuracy: 0.2444\n",
            "Epoch 3/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.6904 - lambda_loss: 0.4397 - dense_3_loss: 0.7978 - dense_3_accuracy: 0.7929\n",
            "Epoch 3: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 18s 74ms/step - loss: 0.6904 - lambda_loss: 0.4397 - dense_3_loss: 0.7978 - dense_3_accuracy: 0.7929 - val_loss: 0.7326 - val_lambda_loss: 0.4309 - val_dense_3_loss: 0.8619 - val_dense_3_accuracy: 0.7722\n",
            "Epoch 4/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.4115 - lambda_loss: 0.4221 - dense_3_loss: 0.4070 - dense_3_accuracy: 0.9109\n",
            "Epoch 4: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 28s 114ms/step - loss: 0.4115 - lambda_loss: 0.4221 - dense_3_loss: 0.4070 - dense_3_accuracy: 0.9109 - val_loss: 0.4993 - val_lambda_loss: 0.4609 - val_dense_3_loss: 0.5158 - val_dense_3_accuracy: 0.8667\n",
            "Epoch 5/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.2891 - lambda_loss: 0.3855 - dense_3_loss: 0.2478 - dense_3_accuracy: 0.9580\n",
            "Epoch 5: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 33s 131ms/step - loss: 0.2891 - lambda_loss: 0.3855 - dense_3_loss: 0.2478 - dense_3_accuracy: 0.9580 - val_loss: 0.3886 - val_lambda_loss: 0.4601 - val_dense_3_loss: 0.3579 - val_dense_3_accuracy: 0.9222\n",
            "Epoch 6/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.2223 - lambda_loss: 0.3492 - dense_3_loss: 0.1679 - dense_3_accuracy: 0.9768\n",
            "Epoch 6: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 31s 124ms/step - loss: 0.2223 - lambda_loss: 0.3492 - dense_3_loss: 0.1679 - dense_3_accuracy: 0.9768 - val_loss: 0.3184 - val_lambda_loss: 0.4546 - val_dense_3_loss: 0.2600 - val_dense_3_accuracy: 0.9167\n",
            "Epoch 7/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.1789 - lambda_loss: 0.3134 - dense_3_loss: 0.1212 - dense_3_accuracy: 0.9865\n",
            "Epoch 7: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 32s 128ms/step - loss: 0.1789 - lambda_loss: 0.3134 - dense_3_loss: 0.1212 - dense_3_accuracy: 0.9865 - val_loss: 0.2674 - val_lambda_loss: 0.4495 - val_dense_3_loss: 0.1893 - val_dense_3_accuracy: 0.9500\n",
            "Epoch 8/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.1496 - lambda_loss: 0.2838 - dense_3_loss: 0.0921 - dense_3_accuracy: 0.9917\n",
            "Epoch 8: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 31s 126ms/step - loss: 0.1496 - lambda_loss: 0.2838 - dense_3_loss: 0.0921 - dense_3_accuracy: 0.9917 - val_loss: 0.2409 - val_lambda_loss: 0.4450 - val_dense_3_loss: 0.1534 - val_dense_3_accuracy: 0.9667\n",
            "Epoch 9/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.1283 - lambda_loss: 0.2566 - dense_3_loss: 0.0733 - dense_3_accuracy: 0.9940\n",
            "Epoch 9: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 36s 143ms/step - loss: 0.1283 - lambda_loss: 0.2566 - dense_3_loss: 0.0733 - dense_3_accuracy: 0.9940 - val_loss: 0.2189 - val_lambda_loss: 0.4234 - val_dense_3_loss: 0.1313 - val_dense_3_accuracy: 0.9778\n",
            "Epoch 10/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.1109 - lambda_loss: 0.2317 - dense_3_loss: 0.0591 - dense_3_accuracy: 0.9960\n",
            "Epoch 10: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 44s 176ms/step - loss: 0.1109 - lambda_loss: 0.2317 - dense_3_loss: 0.0591 - dense_3_accuracy: 0.9960 - val_loss: 0.1988 - val_lambda_loss: 0.4135 - val_dense_3_loss: 0.1068 - val_dense_3_accuracy: 0.9778\n",
            "Epoch 11/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0983 - lambda_loss: 0.2117 - dense_3_loss: 0.0497 - dense_3_accuracy: 0.9965\n",
            "Epoch 11: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 34s 136ms/step - loss: 0.0983 - lambda_loss: 0.2117 - dense_3_loss: 0.0497 - dense_3_accuracy: 0.9965 - val_loss: 0.1914 - val_lambda_loss: 0.4003 - val_dense_3_loss: 0.1019 - val_dense_3_accuracy: 0.9722\n",
            "Epoch 12/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0866 - lambda_loss: 0.1916 - dense_3_loss: 0.0417 - dense_3_accuracy: 0.9974\n",
            "Epoch 12: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 35s 141ms/step - loss: 0.0866 - lambda_loss: 0.1916 - dense_3_loss: 0.0417 - dense_3_accuracy: 0.9974 - val_loss: 0.1711 - val_lambda_loss: 0.3655 - val_dense_3_loss: 0.0877 - val_dense_3_accuracy: 0.9778\n",
            "Epoch 13/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0775 - lambda_loss: 0.1757 - dense_3_loss: 0.0354 - dense_3_accuracy: 0.9981\n",
            "Epoch 13: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 35s 141ms/step - loss: 0.0775 - lambda_loss: 0.1757 - dense_3_loss: 0.0354 - dense_3_accuracy: 0.9981 - val_loss: 0.1439 - val_lambda_loss: 0.3321 - val_dense_3_loss: 0.0632 - val_dense_3_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0710 - lambda_loss: 0.1621 - dense_3_loss: 0.0319 - dense_3_accuracy: 0.9981\n",
            "Epoch 14: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 34s 138ms/step - loss: 0.0710 - lambda_loss: 0.1621 - dense_3_loss: 0.0319 - dense_3_accuracy: 0.9981 - val_loss: 0.1428 - val_lambda_loss: 0.3227 - val_dense_3_loss: 0.0656 - val_dense_3_accuracy: 0.9889\n",
            "Epoch 15/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0637 - lambda_loss: 0.1488 - dense_3_loss: 0.0273 - dense_3_accuracy: 0.9985\n",
            "Epoch 15: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 34s 138ms/step - loss: 0.0637 - lambda_loss: 0.1488 - dense_3_loss: 0.0273 - dense_3_accuracy: 0.9985 - val_loss: 0.1305 - val_lambda_loss: 0.3084 - val_dense_3_loss: 0.0542 - val_dense_3_accuracy: 0.9889\n",
            "Epoch 16/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0575 - lambda_loss: 0.1364 - dense_3_loss: 0.0237 - dense_3_accuracy: 0.9990\n",
            "Epoch 16: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 34s 136ms/step - loss: 0.0575 - lambda_loss: 0.1364 - dense_3_loss: 0.0237 - dense_3_accuracy: 0.9990 - val_loss: 0.1219 - val_lambda_loss: 0.2890 - val_dense_3_loss: 0.0502 - val_dense_3_accuracy: 0.9944\n",
            "Epoch 17/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0524 - lambda_loss: 0.1255 - dense_3_loss: 0.0211 - dense_3_accuracy: 0.9992\n",
            "Epoch 17: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 33s 134ms/step - loss: 0.0524 - lambda_loss: 0.1255 - dense_3_loss: 0.0211 - dense_3_accuracy: 0.9992 - val_loss: 0.1137 - val_lambda_loss: 0.2662 - val_dense_3_loss: 0.0484 - val_dense_3_accuracy: 1.0000\n",
            "Epoch 18/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0482 - lambda_loss: 0.1156 - dense_3_loss: 0.0193 - dense_3_accuracy: 0.9991\n",
            "Epoch 18: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 32s 130ms/step - loss: 0.0482 - lambda_loss: 0.1156 - dense_3_loss: 0.0193 - dense_3_accuracy: 0.9991 - val_loss: 0.1090 - val_lambda_loss: 0.2515 - val_dense_3_loss: 0.0479 - val_dense_3_accuracy: 0.9833\n",
            "Epoch 19/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0456 - lambda_loss: 0.1088 - dense_3_loss: 0.0186 - dense_3_accuracy: 0.9985\n",
            "Epoch 19: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 32s 128ms/step - loss: 0.0456 - lambda_loss: 0.1088 - dense_3_loss: 0.0186 - dense_3_accuracy: 0.9985 - val_loss: 0.1025 - val_lambda_loss: 0.2505 - val_dense_3_loss: 0.0390 - val_dense_3_accuracy: 0.9944\n",
            "Epoch 20/20\n",
            "249/249 [==============================] - ETA: 0s - loss: 0.0407 - lambda_loss: 0.0999 - dense_3_loss: 0.0153 - dense_3_accuracy: 0.9993\n",
            "Epoch 20: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "249/249 [==============================] - 32s 128ms/step - loss: 0.0407 - lambda_loss: 0.0999 - dense_3_loss: 0.0153 - dense_3_accuracy: 0.9993 - val_loss: 0.1085 - val_lambda_loss: 0.2307 - val_dense_3_loss: 0.0561 - val_dense_3_accuracy: 0.9889\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x18a449b29e0>"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_model.fit(\n",
        "    [new_input_a_train, new_input_b_train],  # Inputs\n",
        "    [new_pair_labels_train, new_classification_labels_train],  # Outputs\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_data=([new_input_a_test, new_input_b_test], [pair_labels_test, classification_labels_test]),\n",
        "    callbacks=[cp_callback, es_callback],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxvb2Y299hE3",
        "outputId": "59eb3185-2e37-4b9e-bc9d-ab1b8ac29b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 1s 94ms/step - loss: 0.1085 - lambda_loss: 0.2307 - dense_3_loss: 0.0561 - dense_3_accuracy: 0.9889\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.10845240205526352,\n",
              " 0.23066310584545135,\n",
              " 0.056076373904943466,\n",
              " 0.9888888597488403]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model evaluation\n",
        "combined_model.evaluate([new_input_a_test, new_input_b_test], [pair_labels_test, classification_labels_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Cristina\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# save classifiaction part of the model\n",
        "\n",
        "classification_model.save(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "RBkmDeUW9hE4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "# Loading the saved model\n",
        "model = tf.keras.models.load_model(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFz9Tb0I9hE4",
        "outputId": "1c3b3528-54ae-4ee2-ab04-77429211cbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 258ms/step\n",
            "[5.4041696e-08 2.2859806e-01 6.5231288e-05 7.7045822e-01 3.4549466e-06\n",
            " 1.4148360e-09 6.3952793e-06 7.4088717e-09 6.3129679e-05 8.6830916e-08\n",
            " 8.0538791e-04]\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# Inference test\n",
        "predict_result = model.predict(np.array([input_a_test[0]]))\n",
        "print(np.squeeze(predict_result))\n",
        "print(np.argmax(np.squeeze(predict_result)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3U4yNWx9hE4"
      },
      "source": [
        "# Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "AP1V6SCk9hE5",
        "outputId": "08e41a80-7a4a-4619-8125-ecc371368d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 6ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        15\n",
            "           1       1.00      1.00      1.00        15\n",
            "           2       1.00      1.00      1.00        15\n",
            "           3       1.00      1.00      1.00        15\n",
            "           4       1.00      0.87      0.93        15\n",
            "           5       1.00      1.00      1.00        15\n",
            "           6       1.00      1.00      1.00        15\n",
            "           7       1.00      1.00      1.00        15\n",
            "           8       1.00      1.00      1.00        15\n",
            "           9       1.00      1.00      1.00        15\n",
            "          10       0.94      1.00      0.97        30\n",
            "\n",
            "    accuracy                           0.99       180\n",
            "   macro avg       0.99      0.99      0.99       180\n",
            "weighted avg       0.99      0.99      0.99       180\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def print_confusion_matrix(y_true, y_pred, report=True):\n",
        "    labels = sorted(list(set(y_true)))\n",
        "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    \n",
        "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        " \n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
        "    ax.set_ylim(len(set(y_true)), 0)\n",
        "    plt.show()\n",
        "    \n",
        "    if report:\n",
        "        print('Classification Report')\n",
        "        print(classification_report(classification_labels_test, y_pred))\n",
        "\n",
        "Y_pred = model.predict(new_input_a_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print_confusion_matrix(classification_labels_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNP6aqzc9hE5"
      },
      "source": [
        "# Convert to model for Tensorflow-Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ODjnYyld9hE6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Cristina\\AppData\\Roaming\\Python\\Python310\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save as a model dedicated to inference\n",
        "model.save(model_save_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRfuK8Y59hE6",
        "outputId": "a4ca585c-b5d5-4244-8291-8674063209bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Cristina\\AppData\\Local\\Temp\\tmplb3ms1ao\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Cristina\\AppData\\Local\\Temp\\tmplb3ms1ao\\assets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "21064"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform model (quantization)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBPBXdx9hE6"
      },
      "source": [
        "# Inference test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "mGAzLocO9hE7"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "oQuDK8YS9hE7"
      },
      "outputs": [],
      "source": [
        "# Get I / O tensor\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "2_ixAf_l9hE7"
      },
      "outputs": [],
      "source": [
        "# convert to float32\n",
        "input_a_test = input_a_test.astype(np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], np.array([input_a_test[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4FoAnuc9hE7",
        "outputId": "91f18257-8d8b-4ef3-c558-e9b5f94fabbf",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 3 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Inference implementation\n",
        "interpreter.invoke()\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vONjp19J9hE8",
        "outputId": "77205e24-fd00-42c4-f7b6-e06e527c2cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7.3170391e-08 6.6808808e-01 1.4656565e-04 3.2894814e-01 4.7518784e-05\n",
            " 1.8160679e-09 1.2721411e-05 5.0021178e-08 3.5058038e-05 6.3463386e-07\n",
            " 2.7211499e-03]\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "print(np.squeeze(tflite_results))\n",
        "print(np.argmax(np.squeeze(tflite_results)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "keypoint_classification_EN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
