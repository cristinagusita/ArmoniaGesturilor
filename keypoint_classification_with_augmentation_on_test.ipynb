{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "igMyGnjE9hEp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "RANDOM_SEED = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2HDvhIu9hEr"
      },
      "source": [
        "# Specify each path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9NvZP2Zn9hEy"
      },
      "outputs": [],
      "source": [
        "dataset = 'model/keypoint_classifier/keypoint.csv'\n",
        "model_save_path = 'model/keypoint_classifier/keypoint_classifier.hdf5'\n",
        "tflite_save_path = 'model/keypoint_classifier/keypoint_classifier.tflite'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5oMH7x19hEz"
      },
      "source": [
        "# Set number of classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "du4kodXL9hEz"
      },
      "outputs": [],
      "source": [
        "NUM_CLASSES = 11"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjnL0uso9hEz"
      },
      "source": [
        "# Dataset reading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QT5ZqtEz9hE0"
      },
      "outputs": [],
      "source": [
        "X_dataset = np.loadtxt(dataset, delimiter=',', dtype='float32', usecols=list(range(1, (21 * 2) + 1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QmoKFsp49hE0"
      },
      "outputs": [],
      "source": [
        "y_dataset = np.loadtxt(dataset, delimiter=',', dtype='int32', usecols=(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing data for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Augmenting the data via rotation and shearing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def on_trackbar_change(dummy=None):\n",
        "    # Get current positions of all trackbars\n",
        "    tilt_value = cv2.getTrackbarPos(\"Rotation Angle\", \"Landmarks\") \n",
        "    shear_value_x = cv2.getTrackbarPos(\"Shear_X\", \"Landmarks\") \n",
        "    shear_value_y = cv2.getTrackbarPos(\"Shear_Y\", \"Landmarks\")\n",
        "    # Apply tilt to the landmarks\n",
        "    tilted_landmarks = tilt_keypoints(landmarks, rotation_angle=tilt_value, shear_angle_x=shear_value_x, shear_angle_y=shear_value_y)\n",
        "\n",
        "    # Clear image and redraw landmarks\n",
        "    image[:] = (0, 0, 0)\n",
        "    draw_landmarks(image, tilted_landmarks)\n",
        "    cv2.imshow(\"Landmarks\", image)\n",
        "\n",
        "def draw_landmarks(image, landmarks, color=(0, 255, 0), radius=5):\n",
        "    for (x, y) in landmarks:\n",
        "        cv2.circle(image, (int(x), int(y)), radius, color, -1)\n",
        "    return image\n",
        "\n",
        "def tilt_keypoints(keypoints, rotation_angle, shear_angle_x=0, shear_angle_y=0, center=None):\n",
        "    \"\"\"\n",
        "    Apply a tilt (rotation followed by shear) to keypoints.\n",
        "\n",
        "    :param keypoints: List of (x, y) tuples representing keypoints.\n",
        "    :param rotation_angle: Rotation angle in degrees.\n",
        "    :param shear_angle_x: Shear angle along x-axis in degrees.\n",
        "    :param shear_angle_y: Shear angle along y-axis in degrees.\n",
        "    :param center: A tuple (x, y) representing the center of rotation and shear. \n",
        "                   If None, the mean of keypoints is used.\n",
        "    :return: List of tilted keypoints.\n",
        "    \"\"\"\n",
        "    # Convert angles from degrees to radians\n",
        "    rotation_rad = np.radians(rotation_angle)\n",
        "    shear_rad_x = np.radians(shear_angle_x)\n",
        "    shear_rad_y = np.radians(shear_angle_y)\n",
        "\n",
        "    # Rotation matrix\n",
        "    R = np.array([[np.cos(rotation_rad), -np.sin(rotation_rad)], \n",
        "                  [np.sin(rotation_rad), np.cos(rotation_rad)]])\n",
        "\n",
        "    # Shear matrix\n",
        "    S = np.array([[1, np.tan(shear_rad_x)], \n",
        "                  [np.tan(shear_rad_y), 1]])\n",
        "\n",
        "    # Combined transformation matrix\n",
        "    T = np.dot(S, R)\n",
        "\n",
        "    # If no center is provided, use the mean of keypoints\n",
        "    if center is None:\n",
        "        center = np.mean(keypoints, axis=0)\n",
        "\n",
        "    # Translate keypoints to origin, apply transformation, and translate back\n",
        "    tilted_keypoints = [T.dot(kp - center) + center for kp in keypoints]\n",
        "\n",
        "    return tilted_keypoints\n",
        "\n",
        "image_width, image_height = 640, 480\n",
        "\n",
        "# Original landmarks\n",
        "landmark_list = [[413, 405], [343, 390], [289, 341], [253, 298], [220, 267], [352, 233], [335, 171], [328, 132], [325, 97], [396, 223], [388, 151], [382, 107], [377, 71], [437, 232], [443, 165], [443, 125], [441, 88], [477, 256], [494, 211], [501, 182], [504, 154]]\n",
        "landmarks = [(x, y) for x, y in landmark_list]\n",
        "\n",
        "# Create a black image\n",
        "image = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "\n",
        "# Initialize the window\n",
        "cv2.namedWindow(\"Landmarks\")\n",
        "\n",
        "# Maximum tilt angle for the slider\n",
        "max_tilt = 360\n",
        "max_shear_X = 360\n",
        "max_shear_Y = 360\n",
        "\n",
        "# Create a trackbar (slider) in the window\n",
        "cv2.createTrackbar(\"Rotation Angle\", \"Landmarks\", 0, max_tilt, on_trackbar_change)\n",
        "cv2.createTrackbar(\"Shear_X\", \"Landmarks\", 0, max_shear_X, on_trackbar_change)\n",
        "cv2.createTrackbar(\"Shear_Y\", \"Landmarks\", 0, max_shear_Y, on_trackbar_change)\n",
        "\n",
        "# Initialize the image with original landmarks\n",
        "draw_landmarks(image, landmarks)  # Use 'landmarks' (list of tuples), not 'landmark_list' (list of lists)\n",
        "cv2.imshow(\"Landmarks\", image)\n",
        "\n",
        "# Wait until a key is pressed to exit\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def augment_landmarks(landmarks):\n",
        "    augmented_landmarks = []\n",
        "\n",
        "    # Iterate through the specified ranges\n",
        "    for rotation_angle in range(0, 361, 30):  # [0, 360] with a step of 30\n",
        "        for shear_x in range(-20, 21, 10):      # [-20, 20] with a step of 10\n",
        "            for shear_y in range(-20, 21, 10):  # [-20, 20] with a step of 10\n",
        "                if rotation_angle != 360 and (rotation_angle != 0 and shear_x != 0 and shear_y != 0):\n",
        "                    # Apply the tilt transformation\n",
        "                    tilted_landmarks = tilt_keypoints(landmarks, rotation_angle, shear_x, shear_y)\n",
        "                    tilted_landmarks = np.array([arr.tolist() for arr in tilted_landmarks])\n",
        "                    \n",
        "\n",
        "                    # add guassian noise\n",
        "                    noise = np.random.normal(scale=0.1, size=tilted_landmarks.shape)\n",
        "\n",
        "                    tilted_landmarks = tilted_landmarks + noise\n",
        "\n",
        "                    # convert to list\n",
        "                    tilted_landmarks = [arr.tolist() for arr in tilted_landmarks]\n",
        "\n",
        "                    # Add the result to the list of augmented landmarks\n",
        "                    augmented_landmarks.append(tilted_landmarks)\n",
        "\n",
        "    return augmented_landmarks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import itertools\n",
        "def pre_process_landmark(landmark_list):\n",
        "    # This function does the following:\n",
        "    # Relative Positioning: By normalizing landmarks relative to a specific point on the hand (like the wrist), you account for the hand's position in the image. \n",
        "    # This makes the landmarks' positions relative to each other, rather than to the whole image.\n",
        "    # Scale Invariance: Normalizing the size of landmarks ensures that the gesture recognition is scale-invariant. \n",
        "    # That means the size of the hand (whether it's close to the camera or far away) doesn't affect the recognition process.\n",
        "    # Consistency Across Different Inputs: This process helps in maintaining consistency of the landmark \n",
        "    # data across different frames or different hands, making the gesture recognition more robust and reliable.\n",
        "\n",
        "    landmark_list = landmark_list.reshape(-1, 2).tolist()\n",
        "\n",
        "    temp_landmark_list = copy.deepcopy(landmark_list)\n",
        "\n",
        "    # Convert to relative coordinates\n",
        "    base_x, base_y = 0, 0\n",
        "    for index, landmark_point in enumerate(temp_landmark_list):\n",
        "        if index == 0:\n",
        "            base_x, base_y = landmark_point[0], landmark_point[1]\n",
        "\n",
        "        temp_landmark_list[index][0] = temp_landmark_list[index][0] - base_x\n",
        "        temp_landmark_list[index][1] = temp_landmark_list[index][1] - base_y\n",
        "\n",
        "    # Convert to a one-dimensional list\n",
        "    temp_landmark_list = list(\n",
        "        itertools.chain.from_iterable(temp_landmark_list))\n",
        "\n",
        "    # Normalization\n",
        "    max_value = max(list(map(abs, temp_landmark_list)))\n",
        "\n",
        "    def normalize_(n):\n",
        "        return n / max_value\n",
        "\n",
        "    temp_landmark_list = list(map(normalize_, temp_landmark_list))\n",
        "\n",
        "    return temp_landmark_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize landmarks\n",
        "# new_dataset_X_preprocessed = np.array(list(map(pre_process_landmark, X_dataset)))\n",
        "new_dataset_X_preprocessed = X_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxK_lETT9hE0"
      },
      "source": [
        "# Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vHBmUf1t9hE1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, Lambda\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def initialize_backbone(input_shape):\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(128, activation='tanh', input_shape=input_shape),\n",
        "        Dense(64, activation='tanh'),\n",
        "        Dense(32, activation='tanh'),\n",
        "\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def create_combined_model(input_shape, num_classes, backbone):\n",
        "    # Backbone for feature extraction\n",
        "    input_a = Input(shape=input_shape)\n",
        "    input_b = Input(shape=input_shape)\n",
        "    processed_a = backbone(input_a)\n",
        "    processed_b = backbone(input_b)\n",
        "\n",
        "    # Contrastive Head\n",
        "    distance = Lambda(lambda tensors: K.sqrt(K.sum(K.square(tensors[0] - tensors[1]), axis=1, keepdims=True)))([processed_a, processed_b])\n",
        "\n",
        "\n",
        "    # Classification Head\n",
        "    classification_output = Dense(num_classes, activation='softmax')(processed_a)\n",
        "\n",
        "    # Combined Model\n",
        "    combined_model = Model(inputs=[input_a, input_b], outputs=[distance, classification_output])\n",
        "    \n",
        "    # Classification Model (for inference)\n",
        "    classification_model = Model(inputs=input_a, outputs=classification_output)\n",
        "\n",
        "    return combined_model, classification_model\n",
        "\n",
        "def contrastive_loss(y_true, y_pred):\n",
        "    # print shapes\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    margin = 1 \n",
        "    square_pred = K.square(y_pred)\n",
        "    margin_square = K.square(K.maximum(margin - y_pred, 0))\n",
        "    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n",
        "\n",
        "\n",
        "input_shape = (21 * 2,)  # Example input shape\n",
        "num_classes = NUM_CLASSES  # Define NUM_CLASSES appropriately\n",
        "\n",
        "# Initialize Backbone\n",
        "backbone = initialize_backbone(input_shape)\n",
        "\n",
        "# Create Combined and Classification Models\n",
        "combined_model, classification_model = create_combined_model(input_shape, num_classes, backbone)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MbMjOflQ9hE1"
      },
      "outputs": [],
      "source": [
        "# Model checkpoint callback\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    model_save_path, verbose=1, save_weights_only=False)\n",
        "# Callback for early stopping\n",
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='dense_3_loss', mode='min', patience=5, restore_best_weights=True, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 42)]                 0         []                            \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)        [(None, 42)]                 0         []                            \n",
            "                                                                                                  \n",
            " sequential (Sequential)     (None, 32)                   15840     ['input_1[0][0]',             \n",
            "                                                                     'input_2[0][0]']             \n",
            "                                                                                                  \n",
            " lambda (Lambda)             (None, 1)                    0         ['sequential[0][0]',          \n",
            "                                                                     'sequential[1][0]']          \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 11)                   363       ['sequential[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 16203 (63.29 KB)\n",
            "Trainable params: 16203 (63.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Show summary\n",
        "combined_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 42)]              0         \n",
            "                                                                 \n",
            " sequential (Sequential)     (None, 32)                15840     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 11)                363       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 16203 (63.29 KB)\n",
            "Trainable params: 16203 (63.29 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classification_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "c3Dac0M_9hE2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "combined_model.compile(optimizer='adam',\n",
        "                       loss={'lambda': contrastive_loss, \n",
        "                             'dense_3': 'sparse_categorical_crossentropy'},\n",
        "                       loss_weights={'lambda': 0.3, \n",
        "                                     'dense_3': 0.7},\n",
        "                       metrics={'dense_3': 'accuracy'},\n",
        "                       run_eagerly=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def create_pairs(keypoints, labels):\n",
        "    pair_labels = []\n",
        "    pairs = []\n",
        "    classification_labels = []\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "    label_indices = {label: np.where(labels == label)[0] for label in unique_labels}\n",
        "\n",
        "    for idx, label in enumerate(labels):\n",
        "        # Add classification label\n",
        "        classification_labels.append(label)\n",
        "\n",
        "        # Positive Pair (same label)\n",
        "        positive_indices = label_indices[label]\n",
        "        positive_pair_idx = np.random.choice(positive_indices[positive_indices != idx])\n",
        "        pairs.append([keypoints[idx], keypoints[positive_pair_idx]])\n",
        "        pair_labels.append(1)\n",
        "\n",
        "        # Negative Pair (different label)\n",
        "        negative_label = np.random.choice(unique_labels[unique_labels != label])\n",
        "        negative_pair_idx = np.random.choice(label_indices[negative_label])\n",
        "        pairs.append([keypoints[idx], keypoints[negative_pair_idx]])\n",
        "        pair_labels.append(0)\n",
        "\n",
        "    duplicated_classification_labels = []\n",
        "    for label in labels:\n",
        "        duplicated_classification_labels.extend([label, label])  # Duplicate label for positive and negative pair\n",
        "\n",
        "    return np.array(pairs), np.array(pair_labels), np.array(duplicated_classification_labels)\n",
        "\n",
        "# Example usage\n",
        "pairs, pair_labels, classification_labels = create_pairs(new_dataset_X_preprocessed, y_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split pairs and pair labels with stratification over classification labels\n",
        "pairs_train, pairs_test, pair_labels_train, pair_labels_test, classification_labels_train, classification_labels_test = train_test_split(\n",
        "    pairs, pair_labels, classification_labels, test_size=0.25, stratify=classification_labels, random_state=42)\n",
        "\n",
        "# Splitting the pairs into two inputs for the model\n",
        "input_a_train, input_b_train = pairs_train[:, 0], pairs_train[:, 1]\n",
        "input_a_test, input_b_test = pairs_test[:, 0], pairs_test[:, 1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(270, 90, 270, 90)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(input_a_train), len(input_a_test), len(input_b_train), len(input_b_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given a list [a,b,c,d,e,f], transform it to [[a,b], [c,d], [e,f]]\n",
        "\n",
        "def transform_to_pairs(input_list):\n",
        "    return [input_list[i:i+2] for i in range(0, len(input_list), 2)]\n",
        "\n",
        "# Reverse of the above function\n",
        "def transform_to_list(input_list):\n",
        "    return [elem for pair in input_list for elem in pair]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_input_a_train = []\n",
        "new_classification_labels_train = []\n",
        "new_pair_labels_train = []\n",
        "i = 0\n",
        "for landmark in input_a_train:\n",
        "    a = augment_landmarks(transform_to_pairs(landmark.tolist()))\n",
        "    a = [transform_to_list(elem) for elem in a]\n",
        "    new_input_a_train.append(landmark.tolist())\n",
        "    new_input_a_train.extend(a)\n",
        "    # Add the same classification label for the augmented landmarks\n",
        "    new_classification_labels_train.extend([classification_labels_train[i]] * (len(a) + 1))\n",
        "    # Do the same for pair labels\n",
        "    new_pair_labels_train.extend([pair_labels_train[i]] * (len(a) + 1))\n",
        "    i = i + 1\n",
        "  \n",
        "new_input_a_train = np.array(new_input_a_train)\n",
        "new_classification_labels_train = np.array(new_classification_labels_train)\n",
        "new_pair_labels_train = np.array(new_pair_labels_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_input_b_train = []\n",
        "for landmark in input_b_train:\n",
        "    a = augment_landmarks(transform_to_pairs(landmark.tolist()))\n",
        "    a = [transform_to_list(elem) for elem in a]\n",
        "    new_input_b_train.append(landmark.tolist())\n",
        "    new_input_b_train.extend(a)\n",
        "  \n",
        "new_input_b_train = np.array(new_input_b_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess inputs\n",
        "new_input_a_train = [pre_process_landmark(x) for x in new_input_a_train]\n",
        "new_input_b_train = [pre_process_landmark(x) for x in new_input_b_train]\n",
        "\n",
        "new_input_a_train = np.array(new_input_a_train)\n",
        "new_input_b_train = np.array(new_input_b_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_input_a_test = []\n",
        "new_classification_labels_test = []\n",
        "new_pair_labels_test = []\n",
        "i = 0\n",
        "for landmark in input_a_test:\n",
        "    a = augment_landmarks(transform_to_pairs(landmark.tolist()))\n",
        "    a = [transform_to_list(elem) for elem in a]\n",
        "    new_input_a_test.append(landmark.tolist())\n",
        "    new_input_a_test.extend(a)\n",
        "    # Add the same classification label for the augmented landmarks\n",
        "    new_classification_labels_test.extend([classification_labels_test[i]] * (len(a) + 1))\n",
        "    # Do the same for pair labels\n",
        "    new_pair_labels_test.extend([pair_labels_test[i]] * (len(a) + 1))\n",
        "    i = i + 1\n",
        "  \n",
        "new_input_a_test = np.array(new_input_a_test)\n",
        "new_classification_labels_test = np.array(new_classification_labels_test)\n",
        "new_pair_labels_test = np.array(new_pair_labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_input_b_test = []\n",
        "for landmark in input_b_test:\n",
        "    a = augment_landmarks(transform_to_pairs(landmark.tolist()))\n",
        "    a = [transform_to_list(elem) for elem in a]\n",
        "    new_input_b_test.append(landmark.tolist())\n",
        "    new_input_b_test.extend(a)\n",
        "  \n",
        "new_input_b_test = np.array(new_input_b_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess test\n",
        "new_input_a_test = [pre_process_landmark(x) for x in new_input_a_test]\n",
        "new_input_b_test = [pre_process_landmark(x) for x in new_input_b_test]\n",
        "\n",
        "new_input_a_test = np.array(new_input_a_test)\n",
        "new_input_b_test = np.array(new_input_b_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(False, False, False, False, False, False, False, False)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check for nan values in all the arrays\n",
        "np.isnan(new_input_a_train).any(), np.isnan(new_input_b_train).any(), np.isnan(new_input_a_test).any(), np.isnan(new_input_b_test).any(), np.isnan(new_classification_labels_train).any(), np.isnan(new_pair_labels_train).any(), np.isnan(pair_labels_test).any(), np.isnan(classification_labels_test).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(False, False, False, False, False, False, False, False)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Look for infinity\n",
        "np.isinf(new_input_a_train).any(), np.isinf(new_input_b_train).any(), np.isinf(new_input_a_test).any(), np.isinf(new_input_b_test).any(), np.isinf(new_classification_labels_train).any(), np.isinf(new_pair_labels_train).any(), np.isinf(pair_labels_test).any(), np.isinf(classification_labels_test).any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "360\n",
            "270 90\n"
          ]
        }
      ],
      "source": [
        "print(len(pairs))  # Total number of pairs\n",
        "print(len(pairs_train), len(pairs_test))  # Number of pairs in training and testing datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "47790 47790 47790 47790\n"
          ]
        }
      ],
      "source": [
        "print(len(new_input_a_train), len(new_input_b_train), len(new_classification_labels_train), len(new_pair_labels_train))  # Number of samples in training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(15930, 15930, 15930, 15930)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(new_input_a_test), len(new_input_b_test), len(new_classification_labels_test), len(new_pair_labels_test)  # Number of samples in testing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XI0j1Iu9hE2"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WirBl-JE9hE3",
        "outputId": "71b30ca2-8294-4d9d-8aa2-800d90d399de",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "WARNING:tensorflow:From c:\\Python310\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n",
            "374/374 [==============================] - ETA: 0s - loss: 1.6421 - lambda_loss: 0.1697 - dense_3_loss: 2.2731 - dense_3_accuracy: 0.2240\n",
            "Epoch 1: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 51s 136ms/step - loss: 1.6421 - lambda_loss: 0.1697 - dense_3_loss: 2.2731 - dense_3_accuracy: 0.2240 - val_loss: 1.3909 - val_lambda_loss: 0.2702 - val_dense_3_loss: 1.8712 - val_dense_3_accuracy: 0.3662\n",
            "Epoch 2/20\n",
            "  1/374 [..............................] - ETA: 39s - loss: 1.3849 - lambda_loss: 0.3148 - dense_3_loss: 1.8435 - dense_3_accuracy: 0.4688"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "374/374 [==============================] - ETA: 0s - loss: 0.9516 - lambda_loss: 0.4383 - dense_3_loss: 1.1715 - dense_3_accuracy: 0.6667\n",
            "Epoch 2: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 46s 122ms/step - loss: 0.9516 - lambda_loss: 0.4383 - dense_3_loss: 1.1715 - dense_3_accuracy: 0.6667 - val_loss: 0.5849 - val_lambda_loss: 0.5001 - val_dense_3_loss: 0.6212 - val_dense_3_accuracy: 0.8836\n",
            "Epoch 3/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.4191 - lambda_loss: 0.4627 - dense_3_loss: 0.4003 - dense_3_accuracy: 0.9234\n",
            "Epoch 3: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 45s 121ms/step - loss: 0.4191 - lambda_loss: 0.4627 - dense_3_loss: 0.4003 - dense_3_accuracy: 0.9234 - val_loss: 0.3279 - val_lambda_loss: 0.4755 - val_dense_3_loss: 0.2646 - val_dense_3_accuracy: 0.9583\n",
            "Epoch 4/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.2561 - lambda_loss: 0.3933 - dense_3_loss: 0.1972 - dense_3_accuracy: 0.9715\n",
            "Epoch 4: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 46s 122ms/step - loss: 0.2561 - lambda_loss: 0.3933 - dense_3_loss: 0.1972 - dense_3_accuracy: 0.9715 - val_loss: 0.2233 - val_lambda_loss: 0.4156 - val_dense_3_loss: 0.1409 - val_dense_3_accuracy: 0.9860\n",
            "Epoch 5/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.1847 - lambda_loss: 0.3316 - dense_3_loss: 0.1218 - dense_3_accuracy: 0.9855\n",
            "Epoch 5: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 48s 130ms/step - loss: 0.1847 - lambda_loss: 0.3316 - dense_3_loss: 0.1218 - dense_3_accuracy: 0.9855 - val_loss: 0.1812 - val_lambda_loss: 0.3708 - val_dense_3_loss: 0.0999 - val_dense_3_accuracy: 0.9879\n",
            "Epoch 6/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.1435 - lambda_loss: 0.2830 - dense_3_loss: 0.0837 - dense_3_accuracy: 0.9918\n",
            "Epoch 6: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 48s 127ms/step - loss: 0.1435 - lambda_loss: 0.2830 - dense_3_loss: 0.0837 - dense_3_accuracy: 0.9918 - val_loss: 0.1562 - val_lambda_loss: 0.3356 - val_dense_3_loss: 0.0793 - val_dense_3_accuracy: 0.9833\n",
            "Epoch 7/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.1162 - lambda_loss: 0.2428 - dense_3_loss: 0.0620 - dense_3_accuracy: 0.9947\n",
            "Epoch 7: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 45s 121ms/step - loss: 0.1162 - lambda_loss: 0.2428 - dense_3_loss: 0.0620 - dense_3_accuracy: 0.9947 - val_loss: 0.1276 - val_lambda_loss: 0.2953 - val_dense_3_loss: 0.0558 - val_dense_3_accuracy: 0.9958\n",
            "Epoch 8/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0971 - lambda_loss: 0.2112 - dense_3_loss: 0.0482 - dense_3_accuracy: 0.9959\n",
            "Epoch 8: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 45s 121ms/step - loss: 0.0971 - lambda_loss: 0.2112 - dense_3_loss: 0.0482 - dense_3_accuracy: 0.9959 - val_loss: 0.1077 - val_lambda_loss: 0.2661 - val_dense_3_loss: 0.0399 - val_dense_3_accuracy: 0.9975\n",
            "Epoch 9/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0820 - lambda_loss: 0.1849 - dense_3_loss: 0.0380 - dense_3_accuracy: 0.9975\n",
            "Epoch 9: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 45s 121ms/step - loss: 0.0820 - lambda_loss: 0.1849 - dense_3_loss: 0.0380 - dense_3_accuracy: 0.9975 - val_loss: 0.0921 - val_lambda_loss: 0.2367 - val_dense_3_loss: 0.0302 - val_dense_3_accuracy: 0.9986\n",
            "Epoch 10/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0711 - lambda_loss: 0.1648 - dense_3_loss: 0.0309 - dense_3_accuracy: 0.9980\n",
            "Epoch 10: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 48s 129ms/step - loss: 0.0711 - lambda_loss: 0.1648 - dense_3_loss: 0.0309 - dense_3_accuracy: 0.9980 - val_loss: 0.0861 - val_lambda_loss: 0.2213 - val_dense_3_loss: 0.0282 - val_dense_3_accuracy: 0.9986\n",
            "Epoch 11/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0616 - lambda_loss: 0.1465 - dense_3_loss: 0.0252 - dense_3_accuracy: 0.9986\n",
            "Epoch 11: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 47s 125ms/step - loss: 0.0616 - lambda_loss: 0.1465 - dense_3_loss: 0.0252 - dense_3_accuracy: 0.9986 - val_loss: 0.0766 - val_lambda_loss: 0.2000 - val_dense_3_loss: 0.0238 - val_dense_3_accuracy: 0.9986\n",
            "Epoch 12/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0545 - lambda_loss: 0.1318 - dense_3_loss: 0.0214 - dense_3_accuracy: 0.9989\n",
            "Epoch 12: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 48s 127ms/step - loss: 0.0545 - lambda_loss: 0.1318 - dense_3_loss: 0.0214 - dense_3_accuracy: 0.9989 - val_loss: 0.0733 - val_lambda_loss: 0.1911 - val_dense_3_loss: 0.0228 - val_dense_3_accuracy: 0.9992\n",
            "Epoch 13/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0491 - lambda_loss: 0.1207 - dense_3_loss: 0.0184 - dense_3_accuracy: 0.9989\n",
            "Epoch 13: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 48s 128ms/step - loss: 0.0491 - lambda_loss: 0.1207 - dense_3_loss: 0.0184 - dense_3_accuracy: 0.9989 - val_loss: 0.0636 - val_lambda_loss: 0.1776 - val_dense_3_loss: 0.0147 - val_dense_3_accuracy: 0.9995\n",
            "Epoch 14/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0452 - lambda_loss: 0.1118 - dense_3_loss: 0.0167 - dense_3_accuracy: 0.9988\n",
            "Epoch 14: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 51s 137ms/step - loss: 0.0452 - lambda_loss: 0.1118 - dense_3_loss: 0.0167 - dense_3_accuracy: 0.9988 - val_loss: 0.0586 - val_lambda_loss: 0.1657 - val_dense_3_loss: 0.0128 - val_dense_3_accuracy: 0.9996\n",
            "Epoch 15/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0399 - lambda_loss: 0.1015 - dense_3_loss: 0.0135 - dense_3_accuracy: 0.9994\n",
            "Epoch 15: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 44s 118ms/step - loss: 0.0399 - lambda_loss: 0.1015 - dense_3_loss: 0.0135 - dense_3_accuracy: 0.9994 - val_loss: 0.0573 - val_lambda_loss: 0.1645 - val_dense_3_loss: 0.0113 - val_dense_3_accuracy: 0.9992\n",
            "Epoch 16/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0371 - lambda_loss: 0.0941 - dense_3_loss: 0.0127 - dense_3_accuracy: 0.9992\n",
            "Epoch 16: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 69s 184ms/step - loss: 0.0371 - lambda_loss: 0.0941 - dense_3_loss: 0.0127 - dense_3_accuracy: 0.9992 - val_loss: 0.0542 - val_lambda_loss: 0.1523 - val_dense_3_loss: 0.0121 - val_dense_3_accuracy: 0.9993\n",
            "Epoch 17/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0328 - lambda_loss: 0.0855 - dense_3_loss: 0.0103 - dense_3_accuracy: 0.9996\n",
            "Epoch 17: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 60s 161ms/step - loss: 0.0328 - lambda_loss: 0.0855 - dense_3_loss: 0.0103 - dense_3_accuracy: 0.9996 - val_loss: 0.0507 - val_lambda_loss: 0.1443 - val_dense_3_loss: 0.0105 - val_dense_3_accuracy: 0.9998\n",
            "Epoch 18/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0302 - lambda_loss: 0.0796 - dense_3_loss: 0.0090 - dense_3_accuracy: 0.9997\n",
            "Epoch 18: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 62s 166ms/step - loss: 0.0302 - lambda_loss: 0.0796 - dense_3_loss: 0.0090 - dense_3_accuracy: 0.9997 - val_loss: 0.0455 - val_lambda_loss: 0.1343 - val_dense_3_loss: 0.0074 - val_dense_3_accuracy: 0.9999\n",
            "Epoch 19/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0284 - lambda_loss: 0.0748 - dense_3_loss: 0.0085 - dense_3_accuracy: 0.9998\n",
            "Epoch 19: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 53s 142ms/step - loss: 0.0284 - lambda_loss: 0.0748 - dense_3_loss: 0.0085 - dense_3_accuracy: 0.9998 - val_loss: 0.0427 - val_lambda_loss: 0.1254 - val_dense_3_loss: 0.0072 - val_dense_3_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "374/374 [==============================] - ETA: 0s - loss: 0.0260 - lambda_loss: 0.0692 - dense_3_loss: 0.0075 - dense_3_accuracy: 0.9998\n",
            "Epoch 20: saving model to model/keypoint_classifier\\keypoint_classifier.hdf5\n",
            "374/374 [==============================] - 62s 165ms/step - loss: 0.0260 - lambda_loss: 0.0692 - dense_3_loss: 0.0075 - dense_3_accuracy: 0.9998 - val_loss: 0.0405 - val_lambda_loss: 0.1211 - val_dense_3_loss: 0.0060 - val_dense_3_accuracy: 0.9999\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x1d01a473790>"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "combined_model.fit(\n",
        "    [new_input_a_train, new_input_b_train],  # Inputs\n",
        "    [new_pair_labels_train, new_classification_labels_train],  # Outputs\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_data=([new_input_a_test, new_input_b_test], [new_pair_labels_test, new_classification_labels_test]),\n",
        "    callbacks=[cp_callback, es_callback],\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxvb2Y299hE3",
        "outputId": "59eb3185-2e37-4b9e-bc9d-ab1b8ac29b7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498/498 [==============================] - 33s 67ms/step - loss: 0.0405 - lambda_loss: 0.1211 - dense_3_loss: 0.0060 - dense_3_accuracy: 0.9999\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.04051719605922699,\n",
              " 0.12112532556056976,\n",
              " 0.00597086176276207,\n",
              " 0.999874472618103]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Model evaluation\n",
        "combined_model.evaluate([new_input_a_test, new_input_b_test], [new_pair_labels_test, new_classification_labels_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# save classifiaction part of the model\n",
        "\n",
        "classification_model.save(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "RBkmDeUW9hE4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ]
        }
      ],
      "source": [
        "# Loading the saved model\n",
        "model = tf.keras.models.load_model(model_save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFz9Tb0I9hE4",
        "outputId": "1c3b3528-54ae-4ee2-ab04-77429211cbef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 231ms/step\n",
            "[9.3339826e-04 5.0591153e-09 5.0871962e-01 4.5112274e-09 2.2239310e-10\n",
            " 1.5517742e-04 4.7931188e-01 1.0779953e-02 7.2445424e-08 2.8001774e-09\n",
            " 9.9732759e-05]\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "# Inference test\n",
        "predict_result = model.predict(np.array([input_a_test[0]]))\n",
        "print(np.squeeze(predict_result))\n",
        "print(np.argmax(np.squeeze(predict_result)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3U4yNWx9hE4"
      },
      "source": [
        "# Confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "AP1V6SCk9hE5",
        "outputId": "08e41a80-7a4a-4619-8125-ecc371368d19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "498/498 [==============================] - 2s 4ms/step\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 700x600 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00      1239\n",
            "           1       1.00      1.00      1.00      1239\n",
            "           2       1.00      1.00      1.00      1416\n",
            "           3       1.00      1.00      1.00      1416\n",
            "           4       1.00      1.00      1.00      1416\n",
            "           5       1.00      1.00      1.00      1239\n",
            "           6       1.00      1.00      1.00      1416\n",
            "           7       1.00      1.00      1.00      1239\n",
            "           8       1.00      1.00      1.00      1239\n",
            "           9       1.00      1.00      1.00      1416\n",
            "          10       1.00      1.00      1.00      2655\n",
            "\n",
            "    accuracy                           1.00     15930\n",
            "   macro avg       1.00      1.00      1.00     15930\n",
            "weighted avg       1.00      1.00      1.00     15930\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def print_confusion_matrix(y_true, y_pred, report=True):\n",
        "    labels = sorted(list(set(y_true)))\n",
        "    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    \n",
        "    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n",
        " \n",
        "    fig, ax = plt.subplots(figsize=(7, 6))\n",
        "    sns.heatmap(df_cmx, annot=True, fmt='g' ,square=False)\n",
        "    ax.set_ylim(len(set(y_true)), 0)\n",
        "    plt.show()\n",
        "    \n",
        "    if report:\n",
        "        print('Classification Report')\n",
        "        print(classification_report(new_classification_labels_test, y_pred))\n",
        "\n",
        "Y_pred = model.predict(new_input_a_test)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "print_confusion_matrix(new_classification_labels_test, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNP6aqzc9hE5"
      },
      "source": [
        "# Convert to model for Tensorflow-Lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "ODjnYyld9hE6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save as a model dedicated to inference\n",
        "model.save(model_save_path, include_optimizer=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRfuK8Y59hE6",
        "outputId": "a4ca585c-b5d5-4244-8291-8674063209bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Cristina\\AppData\\Local\\Temp\\tmpfts3y2ds\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\Cristina\\AppData\\Local\\Temp\\tmpfts3y2ds\\assets\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "21064"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Transform model (quantization)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quantized_model = converter.convert()\n",
        "\n",
        "open(tflite_save_path, 'wb').write(tflite_quantized_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHBPBXdx9hE6"
      },
      "source": [
        "# Inference test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "mGAzLocO9hE7"
      },
      "outputs": [],
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=tflite_save_path)\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "oQuDK8YS9hE7"
      },
      "outputs": [],
      "source": [
        "# Get I / O tensor\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2_ixAf_l9hE7"
      },
      "outputs": [],
      "source": [
        "# convert to float32\n",
        "input_a_test = input_a_test.astype(np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], np.array([input_a_test[0]]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4FoAnuc9hE7",
        "outputId": "91f18257-8d8b-4ef3-c558-e9b5f94fabbf",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: total: 0 ns\n",
            "Wall time: 3.02 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Inference implementation\n",
        "interpreter.invoke()\n",
        "tflite_results = interpreter.get_tensor(output_details[0]['index'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vONjp19J9hE8",
        "outputId": "77205e24-fd00-42c4-f7b6-e06e527c2cba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3.0454842e-04 2.1898170e-09 8.2668710e-01 1.3130528e-09 1.6309246e-11\n",
            " 8.0591650e-05 1.6794831e-01 4.9340678e-03 5.9841181e-09 3.0236610e-10\n",
            " 4.5410041e-05]\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "print(np.squeeze(tflite_results))\n",
        "print(np.argmax(np.squeeze(tflite_results)))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "keypoint_classification_EN.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
